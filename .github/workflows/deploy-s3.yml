name: Deploy lacgo-web to S3

on:
  push:
    branches: [ main ]

permissions:
  contents: read
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    environment: s3
    env:
      BUCKET: ${{ secrets.S3_BUCKET }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
      CLOUDFRONT_DISTRIBUTION_ID: ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '22'

      - name: Install dependencies
        run: |
          npm ci

      - name: Build
        run: |
          npm run build

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Ensure build output exists
        run: |
          if [ ! -d dist ]; then
            echo "Build output 'dist' not found. Failing workflow." >&2
            ls -la || true
            exit 1
          fi

      - name: Compress HTML files
        run: |
          # Compress all HTML files with gzip
          find dist -name "*.html" -type f -exec gzip -fk {} \;

      - name: Upload compressed HTML files with metadata
        run: |
          # Upload .html.gz files as .html with gzip encoding
          find dist -name "*.html.gz" -type f | while read gzfile; do
            htmlfile="${gzfile%.gz}"
            s3key="${htmlfile#dist/}"
            
            # Determine cache-control based on filename
            if [[ "$s3key" == "index.html" ]]; then
              CACHE_CONTROL="no-cache, no-store, must-revalidate"
            else
              CACHE_CONTROL="public, max-age=3600"
            fi
            
            aws s3 cp "$gzfile" "s3://$BUCKET/$s3key" \
              --content-type "text/html; charset=utf-8" \
              --content-encoding gzip \
              --cache-control "$CACHE_CONTROL" \
              --metadata-directive REPLACE \
              --region $AWS_REGION
          done

      - name: Sync remaining files to S3
        run: |
          # Sync all non-HTML files
          aws s3 sync dist/ s3://$BUCKET/ --delete --region $AWS_REGION \
            --exclude "*.html" \
            --exclude "*.html.gz"

      - name: Set long Cache-Control for static images
        run: |
          # Set long cache for images
          aws s3 cp --recursive \
            --exclude "*" \
            --include "*.png" \
            --include "*.jpg" \
            --include "*.jpeg" \
            --include "*.svg" \
            --include "*.webp" \
            --include "*.ico" \
            --cache-control "public, max-age=31536000, immutable" \
            --metadata-directive REPLACE \
            s3://$BUCKET/ s3://$BUCKET/ --region $AWS_REGION

      - name: Set long Cache-Control for assets (js, css, fonts, json)
        run: |
          # Set long cache for js, css, fonts; exclude sourcemaps
          aws s3 cp --recursive \
            --exclude "*" \
            --include "*.js" \
            --include "*.css" \
            --include "*.woff" \
            --include "*.woff2" \
            --include "*.ttf" \
            --include "*.otf" \
            --include "*.eot" \
            --include "*.json" \
            --exclude "*.map" \
            --cache-control "public, max-age=31536000, immutable" \
            --metadata-directive REPLACE \
            s3://$BUCKET/ s3://$BUCKET/ --region $AWS_REGION

      - name: CloudFront invalidation (optional)
        if: ${{ env.CLOUDFRONT_DISTRIBUTION_ID != '' }}
        run: |
          # Invalidate only index and root to refresh SPA shell; hashed assets keep long TTL
          aws cloudfront create-invalidation --distribution-id ${{ env.CLOUDFRONT_DISTRIBUTION_ID }} --paths "/index.html" "/"
